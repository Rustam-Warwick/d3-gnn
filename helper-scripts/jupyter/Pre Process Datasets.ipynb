{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea79a907-4d81-447a-a81e-a0925d998822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "# import ogb\n",
    "# from ogb.nodeproppred import NodePropPredDataset\n",
    "import collections\n",
    "import os\n",
    "os.environ[\"DATASET_DIR\"] = \"/Users/rustamwarwick/Documents/Warwick/d3-gnn/datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c8174-3680-4853-8037-d7d588432902",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tag-Ask-Ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d9bb7a-0271-4931-ab6a-ef71452bfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagAskUbuntu:\n",
    "    def __init__(self):\n",
    "        n_vertices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu-nverts.txt\"), header=None)[0].values\n",
    "        simplices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu-simplices.txt\"), header=None)[0].values\n",
    "        n_labels = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu-node-labels.txt\"), header=None, delimiter=\" \", usecols=[1])[1].values\n",
    "        simplex_labels = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu-simplex-labels.txt\"), header=None, delimiter=\" \")[0].values\n",
    "        self.q2t = collections.defaultdict(list) # simplex -> [nodes]\n",
    "        self.t2q = collections.defaultdict(list) # nodes -> [simplex]\n",
    "        index = 0\n",
    "        for simplex_idx in range(n_vertices.shape[0]):\n",
    "            s = str(simplex_labels[simplex_idx])\n",
    "            for j in simplices[index:index+n_vertices[simplex_idx]]:\n",
    "                n_label = n_labels[j-1]\n",
    "                self.t2q[n_label].append(s)\n",
    "                self.q2t[s].append(n_label)\n",
    "            index+=n_vertices[simplex_idx]\n",
    "            \n",
    "    def create_files(self):\n",
    "        def create_file(my_dict, destination):\n",
    "            with open(destination,\"w\") as f:\n",
    "                for key, val in my_dict.items():\n",
    "                    f.write(f'{key},{\",\".join(val)}\\n')\n",
    "        create_file(self.q2t, os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu[question-tag].txt\"))\n",
    "        create_file(self.t2q, os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu[tag-question].txt\"))\n",
    "        \n",
    "    def generate_statistics(self):\n",
    "        num_nodes = len(self.t2q)\n",
    "        num_hyperedges = len(self.q2t)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c90664-ef88-4cf2-95e1-1ad8e668a5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121.25355857915069, 10.28464271614686)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = 0unique i,j in res.iterrows():\n",
    "    acc += res[:i][res[:i][0] == j[1]].shape[0]\n",
    "acc = acc / res.shape[0]\n",
    "acc, res.groupby(0).count()[1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7008431-77d3-4989-a873-c0cbd8e4abd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DBLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43a50d3a-7831-4e4c-bcfc-74f27984f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBLP:\n",
    "    def __init__(self):\n",
    "        n_vertices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-DBLP-full\",\"coauth-DBLP-full-nverts.txt\"), header=None)[0].values\n",
    "        simplices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-DBLP-full\",\"coauth-DBLP-full-simplices.txt\"), header=None)[0].values\n",
    "        self.p2a = collections.defaultdict(list) # simplex[publication] -> nodes[author]\n",
    "        self.a2p = collections.defaultdict(list) # nodes[author] -> simplex[publication]\n",
    "        index = 0\n",
    "        for simplex_idx in range(n_vertices.shape[0]):\n",
    "            ids = \"h\"+str(simplex_idx)\n",
    "            for j in simplices[index:index+n_vertices[simplex_idx]]:\n",
    "                self.a2p[j].append(ids)\n",
    "                self.p2a[ids].append(j)\n",
    "            index+=n_vertices[simplex_idx]\n",
    "            \n",
    "    def create_files(self):\n",
    "        def create_file(my_dict, destination):\n",
    "            with open(destination,\"w\") as f:\n",
    "                for key, val in my_dict.items():\n",
    "                    f.write(f'{str(key)},{\",\".join(map(str, val))}\\n')\n",
    "        create_file(self.p2a, os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-DBLP-full\",\"coauth-DBLP-full[publication-author].txt\"))\n",
    "        create_file(self.a2p, os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-DBLP-full\",\"coauth-DBLP-full[author-publication].txt\"))\n",
    "        \n",
    "    def generate_statistics(self):\n",
    "        return len(self.p2a),len(self.a2p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "604efb76-643d-49b4-befc-a7c5556c26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DBLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73f7ab89-1aea-4425-8431-5a720d7943a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.create_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f2c26-d06c-4044-a261-1dd2ca84ad9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MAG-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c409f0-4c47-4ff8-a450-2400ec5edd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGHistory:\n",
    "    def __init__(self):\n",
    "        n_vertices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-MAG-history\",\"coauth-MAG-history-nverts.txt\"), header=None)[0].values\n",
    "        simplices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-MAG-history\",\"coauth-MAG-history-simplices.txt\"), header=None)[0].values\n",
    "        self.p2a = collections.defaultdict(list) # simplex[publication] -> nodes[author]\n",
    "        self.a2p = collections.defaultdict(list) # nodes[author] -> simplex[publication]\n",
    "        index = 0\n",
    "        for simplex_idx in range(n_vertices.shape[0]):\n",
    "            ids = \"h\"+str(simplex_idx)\n",
    "            for j in simplices[index:index+n_vertices[simplex_idx]]:\n",
    "                self.a2p[j].append(ids)\n",
    "                self.p2a[ids].append(j)\n",
    "            index+=n_vertices[simplex_idx]\n",
    "            \n",
    "    def create_files(self):\n",
    "        def create_file(my_dict, destination):\n",
    "            with open(destination,\"w\") as f:\n",
    "                for key, val in my_dict.items():\n",
    "                    f.write(f'{str(key)},{\",\".join(map(str, val))}\\n')\n",
    "        create_file(self.p2a, os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-MAG-history\",\"coauth-DBLP-full[publication-author].txt\"))\n",
    "        create_file(self.a2p, os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-MAG-history\",\"coauth-MAG-history[author-publication].txt\"))\n",
    "        \n",
    "    def generate_statistics(self):\n",
    "        return len(self.p2a),len(self.a2p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ceff29b-b104-4037-bfad-cf2cbbde30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MAGHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086836d-62e6-4819-8c79-00e340cef5c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Threads-Math-SX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c6135b-050d-4370-bd25-aad8c9affaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadsMathSX:\n",
    "    def __init__(self):\n",
    "        n_vertices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"threads-math-sx\",\"threads-math-sx-nverts.txt\"), header=None)[0].values\n",
    "        simplices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"threads-math-sx\",\"threads-math-sx-simplices.txt\"), header=None)[0].values\n",
    "        self.q2t = collections.defaultdict(list) # simplex[Question] -> nodes[Tag]\n",
    "        self.t2q = collections.defaultdict(list) # nodes[Tag] -> simplex[Question]\n",
    "        index = 0\n",
    "        for simplex_idx in range(n_vertices.shape[0]):\n",
    "            ids = \"h\"+str(simplex_idx)\n",
    "            for j in simplices[index:index+n_vertices[simplex_idx]]:\n",
    "                self.t2q[j].append(ids)\n",
    "                self.q2t[ids].append(j)\n",
    "            index+=n_vertices[simplex_idx]\n",
    "            \n",
    "    def create_files(self):\n",
    "        def create_file(my_dict, destination):\n",
    "            with open(destination,\"w\") as f:\n",
    "                for key, val in my_dict.items():\n",
    "                    f.write(f'{str(key)},{\",\".join(map(str, val))}\\n')\n",
    "        create_file(self.q2t, os.path.join(os.environ[\"DATASET_DIR\"], \"threads-math-sx\",\"threads-math-sx[question-tag].txt\"))\n",
    "        create_file(self.t2q, os.path.join(os.environ[\"DATASET_DIR\"], \"threads-math-sx\",\"threads-math-sx[tag-question].txt\"))\n",
    "        \n",
    "    def generate_statistics(self):\n",
    "        return len(self.p2a),len(self.a2p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "609d8d3c-3055-4472-b62e-a33efb426d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ThreadsMathSX()\n",
    "a.create_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43b0bd-7304-4ea8-b533-156ef7066513",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# OGB-Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3dd2588-8eaa-4f9d-ae4d-ad88a484c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OGBProducts:\n",
    "    def __init__():\n",
    "        dataset = NodePropPredDataset(name = \"ogbn-products\", root = 'dataset/')\n",
    "        shuffled_topology = pd.DataFrame(dataset.graph['edge_index'].T).sample(frac=1)\n",
    "        features = pd.DataFrame(dataset.graph[\"node_feat\"])\n",
    "        labels = pd.DataFrame(dataset.labels)\n",
    "    def save():\n",
    "        shuffled_topology.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"ogb-products\",\"edges.csv\"), header=None, index=False)\n",
    "        features.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"ogb-products\",\"node_features.csv\"), header=None)\n",
    "        labels.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"ogb-products\",\"node_labels.csv\"), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a87cc5-ca4f-4087-b958-1660d3c2b75c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reddit Hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a7fa820-b785-4a1a-9fe4-8a5a40cd9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditHyperlinks:\n",
    "    def __init__(self):\n",
    "            self.dataset_body = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"RedditHyperlinks\", \"soc-redditHyperlinks-body.tsv\"), header=None)\n",
    "            self.dataset_title = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"RedditHyperlinks\", \"soc-redditHyperlinks-title.tsv\"), header=None)\n",
    "    def save(self):\n",
    "        self.dataset_body.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"RedditHyperlinks\", \"soc-redditHyperlinks-body.tsv\"),sep='\\t', index=False, header=False)\n",
    "        self.dataset_title.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"RedditHyperlinks\", \"soc-redditHyperlinks-title.tsv\"),sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7adac2ba-88e9-43a0-b2f7-cbed02e717f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rustamwarwick/miniconda3/envs/flink/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3309: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "a = RedditHyperlinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72912126-a700-4266-9864-47ecfe27f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
