{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea79a907-4d81-447a-a81e-a0925d998822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "import collections\n",
    "import os\n",
    "os.environ[\"DATASET_DIR\"] = \"/Users/rustamwarwick/Documents/Warwick/d3-gnn/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "492df379-4796-4d6a-a27f-d28bfec99566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c8174-3680-4853-8037-d7d588432902",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tag-Ask-Ubuntu 3029 tags, 271233 simplexes, 1468584 star-expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d9bb7a-0271-4931-ab6a-ef71452bfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagAskUbuntu:\n",
    "    def __init__(self):\n",
    "        n_vertices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu-nverts.txt\"), header=None)[0].values\n",
    "        simplices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu-simplices.txt\"), header=None)[0].values\n",
    "        n_labels = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu-node-labels.txt\"), header=None, delimiter=\" \", usecols=[1])[1].values\n",
    "        simplex_labels = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu-simplex-labels.txt\"), header=None, delimiter=\" \")[0].values\n",
    "        self.q2t = collections.defaultdict(list) # simplex -> [nodes]\n",
    "        self.t2q = collections.defaultdict(list) # nodes -> [simplex]\n",
    "        index = 0\n",
    "        for simplex_idx in range(n_vertices.shape[0]):\n",
    "            s = str(simplex_labels[simplex_idx])\n",
    "            for j in simplices[index:index+n_vertices[simplex_idx]]:\n",
    "                n_label = n_labels[j-1]\n",
    "                self.t2q[n_label].append(s)\n",
    "                self.q2t[s].append(n_label)\n",
    "            index+=n_vertices[simplex_idx]\n",
    "            \n",
    "    def create_files(self):\n",
    "        def create_file(my_dict, destination):\n",
    "            with open(destination,\"w\") as f:\n",
    "                for key, val in my_dict.items():\n",
    "                    f.write(f'{key},{\",\".join(val)}\\n')\n",
    "        create_file(self.q2t, os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu[question-tag].txt\"))\n",
    "        create_file(self.t2q, os.path.join(os.environ[\"DATASET_DIR\"], \"tags-ask-ubuntu\",\"tags-ask-ubuntu[tag-question].txt\"))\n",
    "        \n",
    "    def generate_statistics(self):\n",
    "        num_nodes = len(self.t2q)\n",
    "        num_hyperedges = len(self.q2t)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c90664-ef88-4cf2-95e1-1ad8e668a5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121.25355857915069, 10.28464271614686)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = 0unique i,j in res.iterrows():\n",
    "    acc += res[:i][res[:i][0] == j[1]].shape[0]\n",
    "acc = acc / res.shape[0]\n",
    "acc, res.groupby(0).count()[1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cc484-599a-4909-b125-3af4bad072b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7008431-77d3-4989-a873-c0cbd8e4abd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DBLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43a50d3a-7831-4e4c-bcfc-74f27984f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBLP:\n",
    "    def __init__(self):\n",
    "        n_vertices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-DBLP-full\",\"coauth-DBLP-full-nverts.txt\"), header=None)[0].values\n",
    "        simplices = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-DBLP-full\",\"coauth-DBLP-full-simplices.txt\"), header=None)[0].values\n",
    "        self.p2a = collections.defaultdict(list) # simplex[publication] -> nodes[author]\n",
    "        self.a2p = collections.defaultdict(list) # nodes[author] -> simplex[publication]\n",
    "        index = 0\n",
    "        for simplex_idx in range(n_vertices.shape[0]):\n",
    "            ids = \"h\"+str(simplex_idx)\n",
    "            for j in simplices[index:index+n_vertices[simplex_idx]]:\n",
    "                self.a2p[j].append(ids)\n",
    "                self.p2a[ids].append(j)\n",
    "            index+=n_vertices[simplex_idx]\n",
    "            \n",
    "    def create_files(self):\n",
    "        def create_file(my_dict, destination):\n",
    "            with open(destination,\"w\") as f:\n",
    "                for key, val in my_dict.items():\n",
    "                    f.write(f'{str(key)},{\",\".join(map(str, val))}\\n')\n",
    "        create_file(self.p2a, os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-DBLP-full\",\"coauth-DBLP-full[publication-author].txt\"))\n",
    "        create_file(self.a2p, os.path.join(os.environ[\"DATASET_DIR\"], \"coauth-DBLP-full\",\"coauth-DBLP-full[author-publication].txt\"))\n",
    "        \n",
    "    def generate_statistics(self):\n",
    "        return len(self.p2a),len(self.a2p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "604efb76-643d-49b4-befc-a7c5556c26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DBLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73f7ab89-1aea-4425-8431-5a720d7943a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.create_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43b0bd-7304-4ea8-b533-156ef7066513",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# OGB-Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3dd2588-8eaa-4f9d-ae4d-ad88a484c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OGBProducts:\n",
    "    def __init__():\n",
    "        dataset = NodePropPredDataset(name = \"ogbn-products\", root = 'dataset/')\n",
    "        shuffled_topology = pd.DataFrame(dataset.graph['edge_index'].T).sample(frac=1)\n",
    "        features = pd.DataFrame(dataset.graph[\"node_feat\"])\n",
    "        labels = pd.DataFrame(dataset.labels)\n",
    "    def save():\n",
    "        shuffled_topology.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"ogb-products\",\"edges.csv\"), header=None, index=False)\n",
    "        features.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"ogb-products\",\"node_features.csv\"), header=None)\n",
    "        labels.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"ogb-products\",\"node_labels.csv\"), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a87cc5-ca4f-4087-b958-1660d3c2b75c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reddit Hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7a7fa820-b785-4a1a-9fe4-8a5a40cd9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditHyperlinks:\n",
    "    def __init__(self, ending='body', sep=',', usecols=None):\n",
    "        self.ending = ending\n",
    "        self.dataset = pd.read_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"RedditHyperlinks\", f\"soc-redditHyperlinks-{ending}.tsv\"), usecols=usecols, sep=sep, header=None)\n",
    "        nodes = set(a.dataset[0].unique())\n",
    "        nodes.update(a.dataset[1].unique())\n",
    "        self.nodes_2_index = dict(zip(nodes, range(len(nodes))))\n",
    "        \n",
    "    def partition(self, num_partitions):\n",
    "        src = self.dataset[0].map(self.nodes_2_index).values\n",
    "        dest = self.dataset[1].map(self.nodes_2_index).values\n",
    "        adj_list = [[] for _ in range(len(self.nodes_2_index))]\n",
    "        for i in range(len(src)):\n",
    "            found = False\n",
    "            for j in adj_list[src[i]]:\n",
    "                if j[0] == dest[i]:\n",
    "                    j[1]+=1\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                adj_list[src[i]].append([dest[i], 1])\n",
    "        parts = metis.part_graph(adj_list, num_partitions)[1]\n",
    "        self.dataset[3] = self.dataset[0].map(self.nodes_2_index).map(lambda x: parts[x])\n",
    "        return parts\n",
    "    def save(self):\n",
    "        self.dataset.to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"RedditHyperlinks\", f\"soc-redditHyperlinks-{self.ending}.tsv\"),sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7adac2ba-88e9-43a0-b2f7-cbed02e717f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = RedditHyperlinks(sep='\\t', usecols=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d25cc94c-3f67-4f08-a04f-684f70574899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x28c8f7160>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = a.partition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cb9d0b54-d806-4b1f-b4d4-ba5c21ccc9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           teamredditteams\n",
       "1                    soccer\n",
       "2                    bikela\n",
       "3                       cfb\n",
       "4                   gamedev\n",
       "                ...        \n",
       "286556        debatefascism\n",
       "286557            justnomil\n",
       "286558    blackdesertonline\n",
       "286559            askreddit\n",
       "286560      dataisbeautiful\n",
       "Name: 1, Length: 286561, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f182062a-d7fc-4417-a43d-6b7f6e84dc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 1, ..., 2, 5, 1])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(a.dataset[3].values,[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5c22b158-a71a-481b-9d10-b87ae9481119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teamredditteams'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232bdbac-6203-4b40-8cf3-53c6d0386a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44837386-0ffd-4d65-b9a6-727916e7494e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d33fa-1fe9-4867-a8ec-1998ffec68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "npzarr = np.load(os.path.join(os.environ[\"DATASET_DIR\"], \"DGraphFin\", \"dgraphfin.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33b99e5d-d7a2-41bf-aa30-ba30e4e226ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_list_file(npzarr):\n",
    "    order = npzarr['edge_timestamp'].argsort()\n",
    "    edges_ts = np.hstack((npzarr['edge_index'][order], npzarr['edge_timestamp'][order].reshape((1, -1)).T))\n",
    "    pd.DataFrame(edges_ts).to_csv(os.path.join(os.environ[\"DATASET_DIR\"], \"DGraphFin\", \"edge-list.csv\"), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57808c0b-bbae-458e-a524-c9564ed06ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_and_labels(npzarr):\n",
    "    np.save(os.path.join(os.environ[\"DATASET_DIR\"], \"DGraphFin\", \"node_features\"), npzarr['x'].astype(\"float32\"))\n",
    "    np.save(os.path.join(os.environ[\"DATASET_DIR\"], \"DGraphFin\", \"node_labels\"), npzarr['y'].astype(\"int8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc6b39d1-afed-4e85-a667-03edd427395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature_and_labels(npzarr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
